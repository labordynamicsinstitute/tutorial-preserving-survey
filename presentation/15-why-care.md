# Why do we care about this?

# Opening up technical possibilities

## How can we know that a data source is reliably obtained?

## Consider the case of Gino

![Francesca Gino](images/gino-nyt.png)

## The case of Gino

- Francesca Gino was a tenured professor at Harvard Business School, writing on honesty (!)

## The case of Gino

- Several articles were investigated by third parties (Data Colada, in particular [^colada1]), and found to be problematic

::::{.columns}  

::: {.column width="10%"}
:::

::: {.column width="80%"}
![Data manipulated](images/gino-data-colada-118.png)

:::
::: {.column width="10%"}
:::
::::

[^colada1]: <https://datacolada.org/109>, <https://datacolada.org/110>, <https://datacolada.org/111>, <https://datacolada.org/112>, <https://datacolada.org/114>, <https://datacolada.org/118>

## The case of Gino

- At least one of them had manipulated data **AFTER** it had been collected, **BEFORE** it had been analyzed.

:::: {.columns}

::: {.column width="50%"}

![Data manipulation](images/gino-Page-517-Annotated-Screenshot-Cropped.png)

:::

::: {.column width="50%"}

![Results of manipulation](images/gino-data-colada-118-1.png)

:::
::::



## Generic survey processing

::: {.white-col}
![Generic survey processing](images/xkcd_data_provenance-0.png)
:::

Take a generic survey process, with **survey form**,  **dataset** , **processing code** and a **manuscript**.

## Generic survey processing

::: {.white-col}
![Generic survey processing](images/xkcd_data_provenance-1.png)
:::

## Requiring transparency in academia

::: {.white-col}
![Generic survey processing](images/xkcd_data_provenance-2.png)
:::

Data and code sharing policies require that academics provide **dataset** , cleaned by **processing code** leading to a  **manuscript**. 

## Verifying transparency in academia

::: {.white-col}
![Generic survey processing](images/xkcd_data_provenance-3.png)
:::

Journals (and 3rd party services) might even verify  that the **processing code** actually transforms the **dataset** into the results in the **manuscript**.


## Certifiying reproducibility in academia

::: {.white-col}
![Generic survey processing](images/xkcd_data_provenance-9.png)
:::

And they  might even **certify**  reproducibility.[^certify]

[^certify]: Pérignon, Christophe, Kamel Gadouche, Christophe Hurlin, Roxane Silberman, and Eric Debonnel. 2019. “Certify Reproducibility with Confidential Data.” *Science* 365 (6449): 127–28. https://doi.org/10.1126/science.aaw2825 and Jones, Maria. 2024. “Introducing Reproducible Research Standards at the World Bank.” *Harvard Data Science Review* 6 (4). https://doi.org/10.1162/99608f92.21328ce3.



## The missing link: Survey collection

::: {.white-col}
![Generic survey processing](images/xkcd_data_provenance-4.png)
:::

But how can you provide evidence of the flow from the **collection instrument** to the **dataset**?

## This tutorial can help

::: {.white-col}
![Generic survey processing](images/xkcd_data_provenance-5.png)
:::

We will show you here how the automated flow from **collection instrument** to **dataset** can be documented and preserved, lending greater credibility to your research!